{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peer lesson on ARIMA\n",
    "In this notebook I'll attempt to explain what ARIMA is in relation to Time Series forecasting, create a time series dataset from a CSV, visualize and transform the dataset into a time series, and test the data both graphically and with the Dicky-Fuller statistic method. At the recommendation (and assistance) of my boss, a seasonal ARIMA model (SARIMA) is created as well to aid a 10 year forecast/prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "A public dataset of monthly carbon dioxide emissions from electricity generation available at the Energy Information Administration and Jason McNeill. The dataset includes CO2 emissions from each energy resource starting January 1973 to July 2016 for reference and is found [here](https://www.kaggle.com/txtrouble/carbon-emissions/data)\n",
    "I have also included the CSV for this dataset on my GitHub if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is ARIMA?\n",
    "ARIMA is a class of statistical modelling for analyzing and forecasting time series data.\n",
    "It is an acronym that stands for **A**uto**R**egressive **I**ntegrated **M**oving Average. It is a generalization of the simpler AutoRegressive Moving Average and adds the notion of integration. It's used in Statistics, Econometrics, and Time Series Analysis.\n",
    "- 'Autoregression' is model that uses the dependent relationship between an observation and some number of lagged observations (fixed amount of passing time).\n",
    "- 'Integrated' is use of differencing of raw observations (i.e. subtracting an observation from an observation at the previous time step) in order to make the time series stationary.\n",
    "- 'Moving Average' is the model that uses the dependency between an observation and residual errors from a moving average model applied to lagged observations.\n",
    "\n",
    "Non-seasonal ARIMA models are generally denoted as \"ARIMA(p,d,q)\" where p,d,q are non-negative integers. 'p' is the order (the number of time lags), 'd' is the number of times the data have had past values subtracted (also known as the degree of differencing), and 'q' is the order of the moving average. Seasonal ARIMA models are usually denoted ARIMA(p,d,q)(P,D,Q)m, where m refers to the number of periods in each season, and the uppercase P,D,Q refer to the autoregressive, differencing, and moving average terms for the seasonal part of the ARIMA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # specify to ignore warning messages, helps with deprecation\n",
    "rcParams['figure.figsize'] = 17.5, 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing needed libraries, read the CSV and then visualize the data to determine if there's any cleaning or formatting needed before converting to time series (ts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f2a2fcd6d649>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"MER_T12_06.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"MER_T12_06.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the CSV was read as a dataframe, the following arguments are added to the read_csv to convert to a time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateparse = lambda x: pd.to_datetime(x, format='%Y%m', errors = 'coerce')\n",
    "df = pd.read_csv(\"MER_T12_06.csv\", parse_dates=['YYYYMM'], index_col='YYYYMM', date_parser=dateparse) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The added arguments are explained this way:\n",
    "\n",
    "- parse_dates: This is a key to identify the date time column. Example, the column name is ‘YYYYMM’.\n",
    "- index_col: This is a key that forces pandas to use the date time column as index.\n",
    "- date_parser: Converts an input string into datetime variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total sum of CO2 emission from each energy group for every year is given as an observation that can be viewed in the NaT row. So I need to identify the non date/time index rows and then convert the index to datetime, coerce errors, and filter NaT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = df[pd.Series(pd.to_datetime(df.index, errors='coerce')).notnull().values]\n",
    "ts.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ts data type shows that the emission value is represented as an object so the emission value needs to be converted into a numeric value as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts['Value'] = pd.to_numeric(ts['Value'] , errors='coerce')\n",
    "ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there are over 4k observations that have emissions value so we'll need to drop the empty rows emissions value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 8 energy sources of CO2 emission. So we'll group the CO2 Emission dataset by the type of energy source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Energy_sources = ts.groupby('Description')\n",
    "Energy_sources.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the dataset is plotted to visualize the dependency of the emission in the power generation with time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for desc, group in Energy_sources:\n",
    "    group.plot(y='Value', label=desc,ax = ax, title='Carbon Emissions per Energy Source', fontsize = 20)\n",
    "    ax.set_xlabel('Time(Monthly)')\n",
    "    ax.set_ylabel('Carbon Emissions in MMT')\n",
    "    ax.xaxis.label.set_size(20)\n",
    "    ax.yaxis.label.set_size(20)\n",
    "    ax.legend(fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also break the above vizualizatin to show data for each energy source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3,3, figsize = (30, 20))\n",
    "for (desc, group), ax in zip(Energy_sources, axes.flatten()):\n",
    "    group.plot(y='Value',ax = ax, title=desc, fontsize = 18)\n",
    "    ax.set_xlabel('Time(Monthly)')\n",
    "    ax.set_ylabel('Carbon Emissions in MMT')\n",
    "    ax.xaxis.label.set_size(18)\n",
    "    ax.yaxis.label.set_size(18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to clean up the descriptions of the energy sources to make them more presentatable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CO2_per_source = ts.groupby('Description')['Value'].sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using shorter descriptions for the energy sources\n",
    "CO2_per_source.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Geothermal Energy', 'Non-Biomass Waste', 'Petroleum Coke','Distillate Fuel ',\n",
    "        'Residual Fuel Oil', 'Petroleum', 'Natural Gas', 'Coal', 'Total Emissions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a bar chart with the new descriptions. (This shows dramatic contributions to CO2 emissions by source.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (16,9))\n",
    "x_label = cols\n",
    "x_tick = np.arange(len(cols))\n",
    "plt.bar(x_tick, CO2_per_source, align = 'center', alpha = 0.5)\n",
    "fig.suptitle(\"CO2 Emissions by Electric Power Sector\", fontsize= 25)\n",
    "plt.xticks(x_tick, x_label, rotation = 70, fontsize = 20)\n",
    "plt.yticks(fontsize = 20)\n",
    "plt.xlabel('Carbon Emissions in MMT', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to develop a time series model to use for forcasting, we'll slice the data for Natural Gas emissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Emissions = ts.iloc[:,1:]   # Monthly total emissions (mte)\n",
    "Emissions= Emissions.groupby(['Description', pd.Grouper(freq='M')])['Value'].sum().unstack(level = 0)\n",
    "mte = Emissions['Natural Gas Electric Power Sector CO2 Emissions'] # monthly total emissions (mte)\n",
    "mte.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mte.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Stationarity for time series\n",
    "(From Wikipedia) A stationary process is a stochastic (having a random probability distribution) process whose unconditional joint probability distribution does not change when shifted in time. Consequently, parameters such as mean and variance, if they are present, also do not change over time.\n",
    "\n",
    "So, now we can make a plot of our chosen data which will will give us an idea of the overall trend and seasonality of the dataset. If it exists in the dataset, the trend and seasonality are removed from the series to transform the nonstationary dataset into stationary and the residuals can be further analyzed.\n",
    "\n",
    "    Stationarity is an assumption underlying many statistical procedures used in time series analysis, non-stationary data is often transformed to become stationary. The most common cause of violation of stationarity is a trend in the mean, which can be due to either the presence of a unit root or of a deterministic trend. If the nonstationarity is caused by the presence of unit root, stochastic shocks have permanent effects and the process is not mean-reverting. However, if it is caused by a deterministic trend, the process is called a trend stationary process, and stochastic shocks have only transitory effects after which the variable tends toward a deterministically evolving mean.\n",
    "    \n",
    "    A trend stationary process is not strictly stationary, but can easily be transformed into a stationary process by removing the underlying trend, which is solely a function of time. Similarly, processes with one or more unit roots can be made stationary through differencing. An important type of non-stationary process that does not include a trend-like behavior is a cyclostationary process, which is a stochastic process that varies cyclically with time.\n",
    "    (This above explantion courtesy of Kaggle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import statsmodels to run specific tests. Coint (cointegration) is test, using the augmented [Engle-Granger](https://www.statisticshowto.com/engle-granger-test/) two-step cointegration test. adfuller is used to run the augmented [Dickey-Fuller](https://www.statisticshowto.com/adf-augmented-dickey-fuller-test/) test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import coint, adfuller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use plot to graphically test stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A formal way of testing stationarity of a dataset is using plotting the moving average or moving variance and see if the series mean and variance varies with time. This will be first tested using TestStationaryPlot and then it is tested again using the Dickey-Fuller test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestStationaryPlot(ts, plot_label = None):\n",
    "    rol_mean = ts.rolling(window = 12, center = False).mean()\n",
    "    rol_std = ts.rolling(window = 12, center = False).std()\n",
    "    \n",
    "    plt.plot(ts, color = 'blue',label = 'Original Data')\n",
    "    plt.plot(rol_mean, color = 'red', label = 'Rolling Mean')\n",
    "    plt.plot(rol_std, color ='black', label = 'Rolling Std')\n",
    "    plt.xticks(fontsize = 25)\n",
    "    plt.yticks(fontsize = 25)\n",
    "    \n",
    "    plt.xlabel('Time in Years', fontsize = 25)\n",
    "    plt.ylabel('Total Emissions', fontsize = 25)\n",
    "    plt.legend(loc='best', fontsize = 25)\n",
    "    if plot_label is not None:\n",
    "        plt.title('Rolling Mean & Standard Deviation (' + plot_label + ')', fontsize = 25)\n",
    "    else:\n",
    "        plt.title('Rolling Mean & Standard Deviation', fontsize = 25)\n",
    "    plt.show(block= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestStationaryAdfuller(ts, cutoff = 0.01):\n",
    "    ts_test = adfuller(ts, autolag = 'AIC')\n",
    "    ts_test_output = pd.Series(ts_test[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    \n",
    "    for key,value in ts_test[4].items():\n",
    "        ts_test_output['Critical Value (%s)'%key] = value\n",
    "    print(ts_test_output)\n",
    "    \n",
    "    if ts_test[1] <= cutoff:\n",
    "        print(\"Strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root, hence it is stationary\")\n",
    "    else:\n",
    "        print(\"Weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the monthly emissions time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestStationaryPlot(mte, 'unmodified data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestStationaryAdfuller(mte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The emissions mean and the variation in standard deviation (black line) clearly vary with time. This shows that the series has a trend. So, it is not a stationary. Also, the Test Statistic is greater than the critical values with 90%, 95% and 99% confidence levels. So, no evidence to reject the null hypothesis. Therefore, the series is nonstationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll transform the dataset to stationary\n",
    "\n",
    "The most common techniques used to estimate or model trend and then remove it from the time series are:\n",
    "- Aggregation – taking average for a time period like monthly/weekly average\n",
    "- Smoothing – taking rolling averages\n",
    "- Polynomial Fitting – fit a regression model\n",
    "\n",
    "One method is to use Moving Average where the average of 'k' consecutive values depending on the frequency of the series (12 months in this case). (Red line is the rolling mean) We'll also test this as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_avg = mte.rolling(12).mean()\n",
    "plt.plot(mte)\n",
    "plt.plot(moving_avg, color='red')\n",
    "plt.xticks(fontsize = 25)\n",
    "plt.yticks(fontsize = 25)\n",
    "plt.xlabel('Time (years)', fontsize = 25)\n",
    "plt.ylabel('CO2 Emission (MMT)', fontsize = 25)\n",
    "plt.title('CO2 emission from electric power generation', fontsize = 25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mte_moving_avg_diff = mte - moving_avg\n",
    "mte_moving_avg_diff.head(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mte_moving_avg_diff.dropna(inplace=True)\n",
    "TestStationaryPlot(mte_moving_avg_diff, 'moving average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestStationaryAdfuller(mte_moving_avg_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another technique is to use a 'weighted moving average'. This where more recent values are given a higher weight. Since all values are given weights, there should be no missing values and this should work even if used with no previous values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mte_exp_weighted_avg = mte.ewm(halflife=12).mean()\n",
    "plt.plot(mte)\n",
    "plt.plot(mte_exp_weighted_avg, color='red')\n",
    "plt.xticks(fontsize = 25)\n",
    "plt.yticks(fontsize = 25)\n",
    "plt.xlabel('Time (years)', fontsize = 25)\n",
    "plt.ylabel('CO2 Emission (MMT)', fontsize = 25)\n",
    "plt.title('CO2 emission from electric power generation', fontsize = 25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mte_ewma_diff = mte - mte_exp_weighted_avg\n",
    "TestStationaryPlot(mte_ewma_diff, 'exp weighted moving avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestStationaryAdfuller(mte_ewma_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common method of dealing with both trend and seasonality is differencing. This is where we take the difference of the original observation at a particular instant with that at the previous instant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mte_first_difference = mte - mte.shift(1)  \n",
    "TestStationaryPlot(mte_first_difference.dropna(inplace=False), 'differencing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestStationaryAdfuller(mte_first_difference.dropna(inplace=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the seasonal difference is tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mte_seasonal_difference = mte - mte.shift(12)  \n",
    "TestStationaryPlot(mte_seasonal_difference.dropna(inplace=False), 'seasonality difference')\n",
    "TestStationaryAdfuller(mte_seasonal_difference.dropna(inplace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mte_seasonal_first_difference = mte_first_difference - mte_first_difference.shift(12)  \n",
    "TestStationaryPlot(mte_seasonal_first_difference.dropna(inplace=False), 'diff of seasonal diff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestStationaryAdfuller(mte_seasonal_first_difference.dropna(inplace=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason for running the above differencing tests is that they can make the time series dataset more stationary.\n",
    "\n",
    "In this next section, a method called \"decomposing\" is used to eliminate trend and seasonality. What will happen is the following will plot out the trend and seasonality seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "decomposition = seasonal_decompose(mte)\n",
    "\n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "\n",
    "plt.subplot(411)\n",
    "plt.plot(mte, label='Original')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(412)\n",
    "plt.plot(trend, label='Trend')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(413)\n",
    "plt.plot(seasonal,label='Seasonality')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(414)\n",
    "plt.plot(residual, label='Residuals')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the stationarity can be checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mte_decompose = residual\n",
    "mte_decompose.dropna(inplace=True)\n",
    "TestStationaryPlot(mte_decompose, 'decomposing')\n",
    "TestStationaryAdfuller(mte_decompose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section relates to building a SARIMA model and is not relevant for this peer lesson (this section was included under the suggestion by my boss.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "fig = sm.graphics.tsa.plot_acf(mte_seasonal_first_difference.iloc[13:], lags=40, ax=ax1)\n",
    "ax2 = fig.add_subplot(212)\n",
    "fig = sm.graphics.tsa.plot_pacf(mte_seasonal_first_difference.iloc[13:], lags=40, ax=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = d = q = range(0, 2) # Define the p, d and q parameters to take any value between 0 and 2\n",
    "pdq = list(itertools.product(p, d, q)) # Generate all different combinations of p, q and q triplets\n",
    "pdq_x_QDQs = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))] # Generate all different combinations of seasonal p, q and q triplets\n",
    "print('Examples of Seasonal ARIMA parameter combinations for Seasonal ARIMA...')\n",
    "print('SARIMAX: {} x {}'.format(pdq[1], pdq_x_QDQs[1]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[2], pdq_x_QDQs[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in pdq:\n",
    "    for seasonal_param in pdq_x_QDQs:\n",
    "        try:\n",
    "            mod = sm.tsa.statespace.SARIMAX(mte,\n",
    "                                            order=param,\n",
    "                                            seasonal_order=seasonal_param,\n",
    "                                            enforce_stationarity=False,\n",
    "                                            enforce_invertibility=False)\n",
    "            results = mod.fit()\n",
    "            print('ARIMA{}x{} - AIC:{}'.format(param, seasonal_param, results.aic))\n",
    "            if results.mle_retvals is not None and results.mle_retvals['converged'] == False:\n",
    "                print(results.mle_retvals)\n",
    "            aic_results.append(results.aic)\n",
    "        except:\n",
    "            continue\n",
    "aic_results.sort()\n",
    "print('Best AIC found: ', aic_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = sm.tsa.statespace.SARIMAX(mte, \n",
    "                                order=(1,1,1), \n",
    "                                seasonal_order=(0,1,1,12),   \n",
    "                                enforce_stationarity=False,\n",
    "                                enforce_invertibility=False)\n",
    "results = mod.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.resid.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.resid.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.resid.plot(kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot_diagnostics(figsize=(15, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating the prediction model\n",
    "We have obtained a model for our time series that can now be used to produce forecasts. We start by comparing predicted values to real values of the time series, which will help us understand the accuracy of our forecast. The get_prediction() and conf_int() attributes allow us to obtain the values and associated confidence intervals for forecasts of the time series.\n",
    "\n",
    "The dynamic=False argument ensures that we produce one-step ahead forecasts, meaning that forecasts at each point are generated using the full history up to that point.\n",
    "\n",
    "We can plot the real and forecasted values of the CO2 emission time series to assess how well the model fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = results.get_prediction(start = 480, end = 522, dynamic=False)\n",
    "pred_ci = pred.conf_int()\n",
    "pred_ci.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = mte['1973':].plot(label='observed')\n",
    "pred.predicted_mean.plot(ax=ax, label='One-step ahead forecast', alpha=.7)\n",
    "\n",
    "ax.fill_between(pred_ci.index,\n",
    "                pred_ci.iloc[:, 0],\n",
    "                pred_ci.iloc[:, 1], color='r', alpha=.5)\n",
    "\n",
    "ax.set_xlabel('Time (years)')\n",
    "ax.set_ylabel('NG CO2 Emissions')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mte_forecast = pred.predicted_mean\n",
    "mte_truth = mte['2013-01-31':]\n",
    "\n",
    "# Compute the mean square error\n",
    "mse = ((mte_forecast - mte_truth) ** 2).mean()\n",
    "print('The Mean Squared Error (MSE) of the forecast is {}'.format(round(mse, 2)))\n",
    "print('The Root Mean Square Error (RMSE) of the forecast: {:.4f}'\n",
    "      .format(np.sqrt(sum((mte_forecast-mte_truth)**2)/len(mte_forecast))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mte_pred_concat = pd.concat([mte_truth, mte_forecast])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of developing the model is to get a good quality predictive power using dynamic forecast. That is, we use information from the time series up to a certain point, and after that, forecasts are generated using values from previous forecasted time points as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dynamic = results.get_prediction(start=pd.to_datetime('2013-01-31'), dynamic=True, full_results=True)\n",
    "pred_dynamic_ci = pred_dynamic.conf_int()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From, plotting the observed and forecasted values of the time series, we see that the overall forecasts are accurate even when we use the dynamic forecast. All forecasted values (red line) match closely to the original observed (blue line) data, and are well within the confidence intervals of our forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = mte['1973':].plot(label='observed', figsize=(20, 15))\n",
    "pred_dynamic.predicted_mean.plot(label='Dynamic Forecast', ax=ax)\n",
    "\n",
    "ax.fill_between(pred_dynamic_ci.index,\n",
    "                pred_dynamic_ci.iloc[:, 0],\n",
    "                pred_dynamic_ci.iloc[:, 1], \n",
    "                color='r', \n",
    "                alpha=.3)\n",
    "\n",
    "ax.fill_betweenx(ax.get_ylim(), \n",
    "                 pd.to_datetime('2013-01-31'), \n",
    "                 mte.index[-1],\n",
    "                 alpha=.1, zorder=-1)\n",
    "\n",
    "ax.set_xlabel('Time (years)')\n",
    "ax.set_ylabel('CO2 Emissions')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the predicted and true values of our time series\n",
    "mte_forecast = pred_dynamic.predicted_mean\n",
    "mte_original = mte['2013-01-31':]\n",
    "\n",
    "# Compute the mean square error\n",
    "mse = ((mte_forecast - mte_original) ** 2).mean()\n",
    "print('The Mean Squared Error (MSE) of the forecast is {}'.format(round(mse, 2)))\n",
    "print('The Root Mean Square Error (RMSE) of the forecast: {:.4f}'\n",
    "      .format(np.sqrt(sum((mte_forecast-mte_original)**2)/len(mte_forecast))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecast\n",
    "Now that the time series dataset has been run through testing we can apply the model to forecast possible emissions for our chosen energy type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get forecast of 10 years or 120 months steps ahead in future\n",
    "forecast = results.get_forecast(steps= 120)\n",
    "# Get confidence intervals of forecasts\n",
    "forecast_ci = forecast.conf_int()\n",
    "forecast_ci.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = mte.plot(label='observed', figsize=(20, 15))\n",
    "forecast.predicted_mean.plot(ax=ax, label='Forecast')\n",
    "ax.fill_between(forecast_ci.index,\n",
    "                forecast_ci.iloc[:, 0],\n",
    "                forecast_ci.iloc[:, 1], color='g', alpha=.4)\n",
    "ax.set_xlabel('Time (year)')\n",
    "ax.set_ylabel('NG CO2 Emission level')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
